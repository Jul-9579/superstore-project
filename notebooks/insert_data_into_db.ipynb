{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old database deleted.\n",
      "Database and tables created.\n",
      "✅ All data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Delete old database if it exists\n",
    "db_path = \"superstore.db\"\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"Old database deleted.\")\n",
    "\n",
    "# Step 2: Recreate the database and load the schema\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable foreign keys\n",
    "cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# Load schema.sql and execute it\n",
    "with open(\"schema.sql\", \"r\") as f:\n",
    "    schema_sql = f.read()\n",
    "cursor.executescript(schema_sql)\n",
    "print(\"Database and tables created.\")\n",
    "\n",
    "# Step 3: Load the cleaned CSV\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Step 4: Split data into tables\n",
    "customers_df = df[['CustomerID', 'CustomerName', 'Segment']].drop_duplicates()\n",
    "addresses_df = df[['Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "products_df = df[['ProductID', 'Category', 'SubCategory', 'ProductName']].drop_duplicates(subset=['ProductID'])\n",
    "orders_df = df[['OrderID', 'CustomerID', 'OrderDate']].drop_duplicates()\n",
    "order_items_df = df[['OrderID', 'ProductID', 'Sales', 'Quantity', 'Discount', 'Profit']].copy()\n",
    "shipments_df = df[['OrderID', 'AddressID', 'ShipDate', 'ShipMode']].drop_duplicates()\n",
    "\n",
    "# Step 5: Insert data into tables\n",
    "customers_df.to_sql(\"customers\", conn, if_exists=\"append\", index=False)\n",
    "addresses_df.to_sql(\"addresses\", conn, if_exists=\"append\", index=False)\n",
    "products_df.to_sql(\"products\", conn, if_exists=\"append\", index=False)\n",
    "orders_df.to_sql(\"customer_orders\", conn, if_exists=\"append\", index=False)\n",
    "order_items_df.to_sql(\"order_items\", conn, if_exists=\"append\", index=False)\n",
    "shipments_df.to_sql(\"shipments\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ All data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20200cf",
   "metadata": {},
   "source": [
    "## Recreate Shipping and Address tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2b8f3",
   "metadata": {},
   "source": [
    "### Connect to the database and drop the old tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa418e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old 'addresses' and 'shipments' tables deleted.\n"
     ]
    }
   ],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# # Connect to your database\n",
    "# conn = sqlite3.connect(\"superstore.db\")\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Enable foreign keys\n",
    "# cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# # Step 1: Drop old tables\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS addresses;\")\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS shipments;\")\n",
    "# print(\"Old 'addresses' and 'shipments' tables deleted.\")\n",
    "\n",
    "# conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecaaa1b",
   "metadata": {},
   "source": [
    "### Recreate the tables with AddressID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a829a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'addresses' and 'shipments' tables recreated and reinserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Reload cleaned CSV\n",
    "# df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# # 1. Create AddressCombi\n",
    "# df[\"AddressCombi\"] = df.apply(lambda x: f\"{x['City']} {x['State']} {x['Region']} {x['Country']} {x['PostalCode']}\", axis=1)\n",
    "\n",
    "# # 2. Create AddressID\n",
    "# df[\"AddressID\"] = pd.factorize(df[\"AddressCombi\"])[0] + 1  # IDs start at 1\n",
    "\n",
    "# # 3. Prepare new tables\n",
    "# addresses_df = df[['AddressID', 'CustomerID', 'Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "# shipments_df = df[['OrderID', 'ShipDate', 'ShipMode']].drop_duplicates()\n",
    "\n",
    "\n",
    "# # 4. Insert into new tables\n",
    "# addresses_df.to_sql(\"addresses\", conn, if_exists=\"replace\", index=False)\n",
    "# shipments_df.to_sql(\"shipments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"✅ 'addresses' and 'shipments' tables recreated and reinserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8d7ab",
   "metadata": {},
   "source": [
    "### Add AddressID to shipments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e1c83",
   "metadata": {},
   "source": [
    "<!-- ### Add AddressID to shipments -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265bbe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old 'shipments' table deleted.\n",
      "✅ 'shipments' table recreated successfully with AddressID!\n"
     ]
    }
   ],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "\n",
    "# # Connect to the database again\n",
    "# conn = sqlite3.connect(\"superstore.db\")\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Enable foreign keys\n",
    "# cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# # Step 1: Drop the current shipments table\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS shipments;\")\n",
    "# print(\"Old 'shipments' table deleted.\")\n",
    "\n",
    "# # Step 2: Reload cleaned CSV\n",
    "# df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# # Step 3: Recreate AddressID\n",
    "# df[\"AddressCombi\"] = df.apply(lambda x: f\"{x['City']} {x['State']} {x['Region']} {x['Country']} {x['PostalCode']}\", axis=1)\n",
    "# df[\"AddressID\"] = pd.factorize(df[\"AddressCombi\"])[0] + 1\n",
    "\n",
    "# # Step 4: Recreate the shipments table, this time with AddressID\n",
    "# shipments_df = df[['OrderID', 'ShipDate', 'ShipMode', 'AddressID']].drop_duplicates()\n",
    "\n",
    "# # Step 5: Insert into the database\n",
    "# shipments_df.to_sql(\"shipments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"✅ 'shipments' table recreated successfully with AddressID!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd24714",
   "metadata": {},
   "source": [
    "### Verify that all AddressID values in your shipments table actually exist in the addresses table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All AddressID values in 'shipments' are valid and match 'addresses'.\n"
     ]
    }
   ],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "\n",
    "# # Connect to your database\n",
    "# conn = sqlite3.connect(\"superstore.db\")\n",
    "\n",
    "# # Query to check if any AddressID in shipments is missing in addresses\n",
    "# query = \"\"\"\n",
    "# SELECT s.OrderID, s.AddressID\n",
    "# FROM shipments s\n",
    "# LEFT JOIN addresses a ON s.AddressID = a.AddressID\n",
    "# WHERE a.AddressID IS NULL;\n",
    "# \"\"\"\n",
    "\n",
    "# missing_address_ids = pd.read_sql_query(query, conn)\n",
    "\n",
    "# if missing_address_ids.empty:\n",
    "#     print(\"✅ All AddressID values in 'shipments' are valid and match 'addresses'.\")\n",
    "# else:\n",
    "#     print(\"❌ Found mismatches! Here are shipments with missing AddressIDs:\")\n",
    "#     print(missing_address_ids)\n",
    "\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337a6b0",
   "metadata": {},
   "source": [
    "## Recreate database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9680b23",
   "metadata": {},
   "source": [
    "### Clean Data to Ensure Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84a172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Make sure AddressID is unique in the DataFrame\n",
    "# addresses_df = addresses_df.drop_duplicates(subset=['AddressID'])\n",
    "\n",
    "# # Insert into the addresses table\n",
    "# addresses_df.to_sql(\"addresses\", conn, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b97ccb",
   "metadata": {},
   "source": [
    "### Reset AddressID Sequence (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3cced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x238303e46c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cursor.execute(\"DELETE FROM addresses\")  # Delete all existing data\n",
    "# cursor.execute(\"UPDATE sqlite_sequence SET seq = 0 WHERE name = 'addresses'\")  # Reset AUTOINCREMENT counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Old database deleted.\n",
      "🏗️ New database and tables created.\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: addresses.AddressID",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Step 6: Insert into tables\u001b[39;00m\n\u001b[0;32m     46\u001b[0m customers_df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomers\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m addresses_df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddresses\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m products_df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m orders_df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_orders\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[1;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[0;32m   3088\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3089\u001b[0m     name,\n\u001b[0;32m   3090\u001b[0m     con,\n\u001b[0;32m   3091\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m   3092\u001b[0m     if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[0;32m   3093\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3094\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3095\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3096\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3097\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   3098\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    839\u001b[0m     )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[0;32m    843\u001b[0m         frame,\n\u001b[0;32m    844\u001b[0m         name,\n\u001b[0;32m    845\u001b[0m         if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[0;32m    846\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    847\u001b[0m         index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m    848\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    849\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    850\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    851\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    852\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[0;32m    854\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2851\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   2841\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[0;32m   2842\u001b[0m     name,\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2848\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   2849\u001b[0m )\n\u001b[0;32m   2850\u001b[0m table\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m-> 2851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:1119\u001b[0m, in \u001b[0;36mSQLTable.insert\u001b[1;34m(self, chunksize, method)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m chunk_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[1;32m-> 1119\u001b[0m num_inserted \u001b[38;5;241m=\u001b[39m exec_insert(conn, keys, chunk_iter)\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_inserted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\raclo\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2547\u001b[0m, in \u001b[0;36mSQLiteTable._execute_insert\u001b[1;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[0;32m   2545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_insert\u001b[39m(\u001b[38;5;28mself\u001b[39m, conn, keys, data_iter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   2546\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_iter)\n\u001b[1;32m-> 2547\u001b[0m     conn\u001b[38;5;241m.\u001b[39mexecutemany(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_statement(num_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), data_list)\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mrowcount\n",
      "\u001b[1;31mIntegrityError\u001b[0m: UNIQUE constraint failed: addresses.AddressID"
     ]
    }
   ],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Step 1: Delete old database if it exists\n",
    "# db_path = \"superstore.db\"\n",
    "# if os.path.exists(db_path):\n",
    "#     try:\n",
    "#         # Close any existing connection to the database\n",
    "#         if 'conn' in locals() and conn:\n",
    "#             conn.close()\n",
    "#         os.remove(db_path)\n",
    "#         print(\"🗑️ Old database deleted.\")\n",
    "#     except PermissionError as e:\n",
    "#         print(f\"❌ Could not delete the database file: {e}\")\n",
    "\n",
    "# # Step 2: Recreate new database and tables\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Enable foreign keys (important!)\n",
    "# cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# # Load and execute updated schema.sql\n",
    "# with open(\"schema.sql\", \"r\") as f:\n",
    "#     schema_sql = f.read()\n",
    "# cursor.executescript(schema_sql)\n",
    "# print(\"🏗️ New database and tables created.\")\n",
    "\n",
    "# # Step 3: Load cleaned CSV\n",
    "# df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# # Step 4: Prepare AddressID\n",
    "# df[\"AddressCombi\"] = df.apply(lambda x: f\"{x['City']} {x['State']} {x['Region']} {x['Country']} {x['PostalCode']}\", axis=1)\n",
    "# df[\"AddressID\"] = pd.factorize(df[\"AddressCombi\"])[0] + 1  # IDs start at 1\n",
    "\n",
    "# # Step 5: Create DataFrames\n",
    "# customers_df = df[['CustomerID', 'CustomerName', 'Segment']].drop_duplicates()\n",
    "# addresses_df = df[['AddressID', 'CustomerID', 'Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "# products_df = df[['ProductID', 'Category', 'SubCategory', 'ProductName']].drop_duplicates(subset=['ProductID'])\n",
    "# orders_df = df[['OrderID', 'CustomerID', 'OrderDate']].drop_duplicates()\n",
    "# order_items_df = df[['OrderID', 'ProductID', 'Sales', 'Quantity', 'Discount', 'Profit']].copy()\n",
    "# shipments_df = df[['OrderID', 'ShipDate', 'ShipMode', 'AddressID']].drop_duplicates()\n",
    "\n",
    "# # Step 6: Insert into tables\n",
    "# customers_df.to_sql(\"customers\", conn, if_exists=\"append\", index=False)\n",
    "# addresses_df.to_sql(\"addresses\", conn, if_exists=\"append\", index=False)\n",
    "# products_df.to_sql(\"products\", conn, if_exists=\"append\", index=False)\n",
    "# orders_df.to_sql(\"customer_orders\", conn, if_exists=\"append\", index=False)\n",
    "# order_items_df.to_sql(\"order_items\", conn, if_exists=\"append\", index=False)\n",
    "# shipments_df.to_sql(\"shipments\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"✅ All data inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-check for Duplicates\n",
    "# addresses_df = addresses_df.drop_duplicates(subset=['AddressID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "9362\n"
     ]
    }
   ],
   "source": [
    "# # Check the unique AddressCombi values\n",
    "# print(df[\"AddressCombi\"].nunique())  # This should show a large number of unique addresses\n",
    "\n",
    "# # Check if there are duplicates in AddressID\n",
    "# print(df[\"AddressID\"].duplicated().sum())  # This should show 0 if no duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cb645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Old database deleted.\n",
      "🏗️ New database and tables created.\n",
      "✅ All data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Step 1: Delete old database if it exists\n",
    "# db_path = \"superstore.db\"\n",
    "# if os.path.exists(db_path):\n",
    "#     try:\n",
    "#         if 'conn' in locals() and conn:\n",
    "#             conn.close()\n",
    "#         os.remove(db_path)\n",
    "#         print(\"🗑️ Old database deleted.\")\n",
    "#     except PermissionError as e:\n",
    "#         print(f\"❌ Could not delete the database file: {e}\")\n",
    "\n",
    "# # Step 2: Recreate new database and tables\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Enable foreign keys (important!)\n",
    "# cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# # Load and execute updated schema.sql\n",
    "# with open(\"schema.sql\", \"r\") as f:\n",
    "#     schema_sql = f.read()\n",
    "# cursor.executescript(schema_sql)\n",
    "# print(\"🏗️ New database and tables created.\")\n",
    "\n",
    "# # Step 3: Load cleaned CSV\n",
    "# df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# # Step 4: Prepare AddressCombi and AddressID\n",
    "# df[\"AddressCombi\"] = df.apply(lambda x: f\"{x['City']} {x['State']} {x['Region']} {x['Country']} {x['PostalCode']}\", axis=1)\n",
    "\n",
    "# # Ensure AddressID is unique\n",
    "# df[\"AddressID\"] = df.index + 1  # Using the index to generate a unique AddressID\n",
    "\n",
    "# # Step 5: Create DataFrames\n",
    "# # Ensure there are no duplicate CustomerID values\n",
    "# customers_df = df[['CustomerID', 'CustomerName', 'Segment']].drop_duplicates(subset=['CustomerID'])\n",
    "# products_df = df[['ProductID', 'Category', 'SubCategory', 'ProductName']].drop_duplicates(subset=['ProductID'])\n",
    "# orders_df = df[['OrderID', 'CustomerID', 'OrderDate']].drop_duplicates()\n",
    "# order_items_df = df[['OrderID', 'ProductID', 'Sales', 'Quantity', 'Discount', 'Profit']].copy()\n",
    "# # Aggregate shipments_df to ensure unique OrderID values\n",
    "# shipments_df = df.groupby('OrderID').agg({\n",
    "#     'ShipDate': 'first',  # Use the first ShipDate for each OrderID\n",
    "#     'ShipMode': 'first',  # Use the first ShipMode for each OrderID\n",
    "#     'AddressID': 'first'  # Use the first AddressID for each OrderID\n",
    "# }).reset_index()\n",
    "\n",
    "# # Step 6: Insert into tables\n",
    "# customers_df.to_sql(\"customers\", conn, if_exists=\"append\", index=False)\n",
    "# addresses_df = df[['AddressID', 'CustomerID', 'Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "# addresses_df.to_sql(\"addresses\", conn, if_exists=\"append\", index=False)\n",
    "# products_df.to_sql(\"products\", conn, if_exists=\"append\", index=False)\n",
    "# orders_df.to_sql(\"customer_orders\", conn, if_exists=\"append\", index=False)\n",
    "# order_items_df.to_sql(\"order_items\", conn, if_exists=\"append\", index=False)\n",
    "# shipments_df.to_sql(\"shipments\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# # Step 7: Commit and close connection\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"✅ All data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e16163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old database deleted.\n",
      "Database and tables created.\n",
      "✅ All data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Delete old database if it exists\n",
    "db_path = \"superstore.db\"\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"Old database deleted.\")\n",
    "\n",
    "# Step 2: Recreate the database and load the schema\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable foreign keys\n",
    "cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# Load schema.sql and execute it\n",
    "with open(\"schema.sql\", \"r\") as f:\n",
    "    schema_sql = f.read()\n",
    "cursor.executescript(schema_sql)\n",
    "print(\"Database and tables created.\")\n",
    "\n",
    "# Step 3: Load the cleaned CSV\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Step 4: Split data into tables\n",
    "customers_df = df[['CustomerID', 'CustomerName', 'Segment']].drop_duplicates()\n",
    "addresses_df = df[['Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "products_df = df[['ProductID', 'Category', 'SubCategory', 'ProductName']].drop_duplicates(subset=['ProductID'])\n",
    "orders_df = df[['OrderID', 'CustomerID', 'OrderDate']].drop_duplicates()\n",
    "order_items_df = df[['OrderID', 'ProductID', 'Sales', 'Quantity', 'Discount', 'Profit']].copy()\n",
    "\n",
    "# Step 5: Insert data into tables except shipments (for now)\n",
    "customers_df.to_sql(\"customers\", conn, if_exists=\"append\", index=False)\n",
    "addresses_df.to_sql(\"addresses\", conn, if_exists=\"append\", index=False)\n",
    "products_df.to_sql(\"products\", conn, if_exists=\"append\", index=False)\n",
    "orders_df.to_sql(\"customer_orders\", conn, if_exists=\"append\", index=False)\n",
    "order_items_df.to_sql(\"order_items\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Step 6: Now prepare the shipments table\n",
    "# Fetch addresses with their newly assigned AddressID\n",
    "addresses_in_db = pd.read_sql_query(\"SELECT AddressID, Country, City, State, PostalCode, Region FROM addresses\", conn)\n",
    "\n",
    "# Prepare shipments dataframe\n",
    "shipments_df = df[['OrderID', 'ShipDate', 'ShipMode', 'Country', 'City', 'State', 'PostalCode', 'Region']].drop_duplicates()\n",
    "\n",
    "# 🔥 Force PostalCode to string type in both\n",
    "shipments_df['PostalCode'] = shipments_df['PostalCode'].astype(str)\n",
    "addresses_in_db['PostalCode'] = addresses_in_db['PostalCode'].astype(str)\n",
    "\n",
    "# Merge shipments with addresses_in_db to get AddressID\n",
    "shipments_df = shipments_df.merge(addresses_in_db, on=['Country', 'City', 'State', 'PostalCode', 'Region'], how='left')\n",
    "\n",
    "# Now keep only the correct columns for inserting into the 'shipments' table\n",
    "shipments_final_df = shipments_df[['OrderID', 'ShipDate', 'ShipMode', 'AddressID']]\n",
    "\n",
    "# Insert into shipments\n",
    "shipments_final_df.to_sql(\"shipments\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ All data inserted successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
